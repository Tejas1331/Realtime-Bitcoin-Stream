# ğŸ“Š Real-Time Crypto Price Forecasting and Aggregation using Kafka, Spark, LSTM, and Streamlit

This project demonstrates an end-to-end real-time data pipeline and forecasting system for cryptocurrency prices (e.g., Bitcoin) using a combination of **Apache Kafka**, **Apache Spark**, **Google Cloud Platform**, **Hadoop HDFS**, **Hive**, **LSTM neural networks**, and **Streamlit**.

The system is capable of:
- Streaming real-time price data from Binance.
- Processing and aggregating data using Spark on GCP.
- Forecasting prices using an LSTM model.
- Storing data in HDFS and Google Sheets.
- Visualizing live data and predictions in Streamlit.

---

## ğŸ“ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ 1_Hadoop_Commands/
â”‚   â””â”€â”€ hadoop_commands.txt               # Basic Hadoop commands used during the project
â”‚
â”œâ”€â”€ 2_Hive_Commands/
â”‚   â””â”€â”€ hive_table_creation.hql          # HiveQL commands to create and manage tables
â”‚
â”œâ”€â”€ 3_Kafka_Schema/
â”‚   â””â”€â”€ schema.avsc                      # Avro schema for Kafka key and value messages
â”‚
â”œâ”€â”€ 4_LSTM_Model/
â”‚   â”œâ”€â”€ preprocess.py                    # Data preprocessing scripts
â”‚   â”œâ”€â”€ train_lstm.py                    # Code to train LSTM model
â”‚   â””â”€â”€ model_utils.py                   # Helper functions for the LSTM model
â”‚
â”œâ”€â”€ 5_Kafka_Spark_HDFS_Integration/
â”‚   â”œâ”€â”€ kafka_consumer.py                # Consumes Kafka stream and processes it with Spark
â”‚   â””â”€â”€ spark_hdfs_sink.py               # Dumps aggregated results into HDFS
â”‚
â”œâ”€â”€ 6_RealTime_Streaming/
â”‚   â”œâ”€â”€ binance_producer.py              # Streams real-time price data from Binance to Kafka
â”‚   â”œâ”€â”€ spark_aggregator_gcp.py          # Spark consumer that aggregates and stores in Google Sheets
â”‚   â””â”€â”€ streamlit_dashboard.py           # Visual dashboard showing live price and predictions
â”‚
â””â”€â”€ README.md                            # This master README
```

---

## ğŸš€ Tech Stack

| Component               | Technology Used                   |
|------------------------|-----------------------------------|
| Data Source            | Binance API                       |
| Messaging Queue        | Apache Kafka                      |
| Stream Processing      | Apache Spark (running on GCP)     |
| Storage                | HDFS, Google Sheets               |
| Model                  | LSTM Neural Network (TensorFlow/Keras) |
| Visualization          | Streamlit                         |
| Query Layer            | Apache Hive                       |
| Orchestration          | Manual + Custom Scripts           |

---

## ğŸ“Œ Key Features

- **Modular architecture** with separate components for ingestion, processing, modeling, and visualization.
- **Real-time aggregation** and prediction.
- **Multiple sinks**: Data is stored in both HDFS (for querying via Hive) and Google Sheets (for easy sharing).
- **Visualization** built with Streamlit for end-users to see predictions and live data.

---

## âš™ï¸ Setup & Requirements

Due to dependencies on Kafka, Spark, GCP, and HDFS, this project requires the following setup (not plug-and-play):

- Kafka broker running (local or on GCP)
- Spark environment configured (GCP or local)
- HDFS cluster running
- Hive installed and connected to HDFS
- Binance API key (if using authenticated endpoints)
- Google Sheets API credentials (for write access)
- Python (with libraries: `pyspark`, `kafka-python`, `streamlit`, `tensorflow`, etc.)

---

## ğŸ” Legal & Compliance

- Binanceâ€™s API can be used for free, and [as per Binanceâ€™s terms](https://binance-docs.github.io/apidocs/spot/en/#general-info), you're allowed to access public market data for research and non-commercial purposes.
- Do **not** use this for live trading or financial advice. This is a demo project for educational purposes only.

---

## ğŸ“ License

This project is for educational and non-commercial use. You are free to fork, learn from, and build upon this for your own research.

---
