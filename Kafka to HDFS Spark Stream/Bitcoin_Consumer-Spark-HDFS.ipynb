{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8354fe38-ffce-48bd-b685-8340daa20d44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 13:41:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BitcoinDataConsumer\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db19429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting confluent_kafka\n",
      "  Downloading confluent_kafka-2.9.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
      "Downloading confluent_kafka-2.9.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: confluent_kafka\n",
      "Successfully installed confluent_kafka-2.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4029ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httpx\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/miniconda3/lib/python3.11/site-packages (from httpx) (3.7.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/miniconda3/lib/python3.11/site-packages (from httpx) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/miniconda3/lib/python3.11/site-packages (from httpx) (3.4)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/miniconda3/lib/python3.11/site-packages (from anyio->httpx) (1.3.1)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h11, httpcore, httpx\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.28.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad2572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting authlib\n",
      "  Downloading authlib-1.5.2-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: cryptography in /opt/conda/miniconda3/lib/python3.11/site-packages (from authlib) (41.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/miniconda3/lib/python3.11/site-packages (from cryptography->authlib) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/miniconda3/lib/python3.11/site-packages (from cffi>=1.12->cryptography->authlib) (2.21)\n",
      "Downloading authlib-1.5.2-py2.py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.1/232.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: authlib\n",
      "Successfully installed authlib-1.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install authlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f09f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "from confluent_kafka import DeserializingConsumer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer\n",
    "from confluent_kafka.serialization import StringDeserializer\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Define Kafka configuration\n",
    "kafka_config = {\n",
    "    'bootstrap.servers': 'Cluster-server',\n",
    "    'sasl.mechanisms': 'PLAIN',\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.username': 'Cluster-username',\n",
    "    'sasl.password': 'Cluster-password',\n",
    "    'group.id': '5'\n",
    "}\n",
    "\n",
    "# Create a Schema Registry client\n",
    "schema_registry_client = SchemaRegistryClient({\n",
    "  'url': 'Schema-registry-endpoint',\n",
    "  'basic.auth.user.info': '{}:{}'.format('Schema-registry-username', 'Schema-registry-password')\n",
    "})\n",
    "\n",
    "# Fetch the latest Avro schema for the value\n",
    "subject_name = 'bitcoin_stream-value'\n",
    "schema_str = schema_registry_client.get_latest_version(subject_name).schema.schema_str\n",
    "\n",
    "# Create Avro Deserializer for the value\n",
    "key_deserializer = StringDeserializer('utf_8')\n",
    "avro_deserializer = AvroDeserializer(schema_registry_client, schema_str)\n",
    "\n",
    "# Define the DeserializingConsumer\n",
    "consumer = DeserializingConsumer({\n",
    "    'bootstrap.servers': kafka_config['bootstrap.servers'],\n",
    "    'security.protocol': kafka_config['security.protocol'],\n",
    "    'sasl.mechanisms': kafka_config['sasl.mechanisms'],\n",
    "    'sasl.username': kafka_config['sasl.username'],\n",
    "    'sasl.password': kafka_config['sasl.password'],\n",
    "    'key.deserializer': key_deserializer,\n",
    "    'value.deserializer': avro_deserializer,\n",
    "    'group.id' : kafka_config['group.id'],\n",
    "    'auto.offset.reset': 'latest',\n",
    "    'enable.auto.commit': True,\n",
    "    'auto.commit.interval.ms': 5000 # Commit every 5000 ms, i.e., every 5 seconds\n",
    "})\n",
    "\n",
    "# Subscribe to the 'retail_data' topic\n",
    "consumer.subscribe(['bitcoin_stream'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "buffer = []\n",
    "current_minute = None\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print('Consumer error: {}'.format(msg.error()))\n",
    "            continue\n",
    "\n",
    "        msg_data = msg.value()\n",
    "        timestamp_str = msg_data.get(\"timestamp\")\n",
    "        price = float(msg_data.get(\"price\"))\n",
    "        dt = datetime.fromisoformat(timestamp_str)\n",
    "\n",
    "        # Create a unique key for this minute\n",
    "        minute_key = (dt.year, dt.month, dt.day, dt.hour, dt.minute)\n",
    "\n",
    "        if current_minute is None:\n",
    "            current_minute = minute_key\n",
    "\n",
    "        if minute_key == current_minute:\n",
    "            buffer.append(price)\n",
    "        else:\n",
    "            # New minute has started, process the buffer\n",
    "            if buffer:\n",
    "                avg_price = sum(buffer) / len(buffer)\n",
    "                year, month, day, hour, minute = current_minute\n",
    "\n",
    "                row = [(year, month, day, hour, minute, avg_price)]\n",
    "                columns = [\"year\", \"month\", \"day\", \"hour\", \"minute\", \"price\"]\n",
    "                df = spark.createDataFrame(row, columns)\n",
    "\n",
    "                df.write \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .partitionBy(\"year\", \"month\", \"day\", \"hour\") \\\n",
    "                    .format(\"parquet\") \\\n",
    "                    .save(\"hdfs:///bitcoin_stream_new\")\n",
    "\n",
    "                print(f\"[{year}-{month:02d}-{day:02d} {hour:02d}:{minute:02d}] Avg price: {avg_price:.2f} written to HDFS.\")\n",
    "\n",
    "            # Reset for the new minute\n",
    "            buffer = [price]\n",
    "            current_minute = minute_key\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    if buffer:\n",
    "        avg_price = sum(buffer) / len(buffer)\n",
    "        year, month, day, hour, minute = current_minute\n",
    "\n",
    "        row = [(year, month, day, hour, minute, avg_price)]\n",
    "        columns = [\"year\", \"month\", \"day\", \"hour\", \"minute\", \"price\"]\n",
    "        df = spark.createDataFrame(row, columns)\n",
    "\n",
    "        df.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"year\", \"month\", \"day\", \"hour\") \\\n",
    "            .format(\"parquet\") \\\n",
    "            .save(\"hdfs:///bitcoin_stream_new\")\n",
    "\n",
    "        print(f\"[{year}-{month:02d}-{day:02d} {hour:02d}:{minute:02d}] Final avg price written before shutdown.\")\n",
    "\n",
    "    consumer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
